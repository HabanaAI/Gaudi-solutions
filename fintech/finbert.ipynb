{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinBERT: Financial Sentiment Analysis using Language Models\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction and Setup](#introduction)\n",
    "- [BERT Fine-Tuning on Single Gaudi Card](#bert-fine-tuning-on-single-gaudi-card)\n",
    "- [Fine-Tuning on Multiple Gaudis](#fine-tuning-on-multiple-gaudis)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook shows how to train [FinBERT](https://arxiv.org/abs/1908.10063) on a Gaudi system. FinBERT is built by further training the BERT language model on a dataset in the finance domain and thus fine-tuning it for financial sentiment classification. Here, we will demonstrain how to use the [Financial PhraseBank](https://arxiv.org/abs/1307.5336v2) dataset for this fine-tuning. You will see how easy and fast it is to train this model on a Gaudi system and how you can further accelerate it using multiple Gaudis. \n",
    "\n",
    "## Setup\n",
    "\n",
    "This notebook runs on a Gaudi system with SynapseAI and Gaudi drivers installed. Please refer to [installation guide](https://docs.habana.ai/en/latest/Installation_Guide/index.html) for more information.\n",
    "We also use the transformers library from Habana's [Model Reference](https://github.com/HabanaAI/Model-References/tree/master/PyTorch/nlp/finetuning/huggingface/bert/#setup) repository.\n",
    "\n",
    "In addition, you need to download and extract Financial PhraseBank dataset from [ResearchGrate](https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10).\n",
    "\n",
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install numpy pandas scikit-learn datasets\n",
    "!git clone -b 1.5.0 https://github.com/HabanaAI/Model-References.git\n",
    "%pip install Model-References/PyTorch/nlp/finetuning/huggingface/bert/transformers/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Fine-Tuning on Single Gaudi Card\n",
    "\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import Dataset\n",
    "\n",
    "# transformers library from Habana's Model Reference repository tailored for Gaudi\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer \n",
    "from transformers import Trainer, TrainingArguments, TextClassificationPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Load Habana-related Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Habana modules from /usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/lib\n"
     ]
    }
   ],
   "source": [
    "from habana_frameworks.torch.utils.library_loader import load_habana_module\n",
    "load_habana_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Input Data\n",
    "\n",
    "Each data point in the Financial PhraseBank dataset consists of financial text and label corresponding to sentiment of the text. The sentiment labels are as follows: 0 for _neutral_, 1 for _positive_, and 2 for _negative_ sentiments.\n",
    "\n",
    "The workflow for loading and preparing input data is shown in the following diagram:\n",
    "\n",
    "![workflow.svg](workflow.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  According to Gran , the company has no plans t...      0\n",
       "1  Technopolis plans to develop in stages an area...      0\n",
       "2  The international electronic industry company ...      2\n",
       "3  With the new production plant the company woul...      1\n",
       "4  According to the company 's updated strategy f...      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    'FinancialPhraseBank-v1.0/Sentences_50Agree.txt',\n",
    "    sep='@',\n",
    "    names=['sentence', 'label'],\n",
    "    encoding = \"ISO-8859-1\")\n",
    "df = df.dropna()\n",
    "df['label'] = df['label'].map({\"neutral\": 0, \"positive\": 1, \"negative\": 2})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_test, = train_test_split(df, stratify=df['label'], test_size=0.1, random_state=42)\n",
    "df_train, df_val = train_test_split(df_train, stratify=df_train['label'],test_size=0.1, random_state=42)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train, preserve_index=False)\n",
    "dataset_val = Dataset.from_pandas(df_val, preserve_index=False)\n",
    "dataset_test = Dataset.from_pandas(df_test, preserve_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model\n",
    "\n",
    "BERT model card is available on [HuggingFace](https://huggingface.co/bert-large-uncased). `BertForSequenceClassification` adds classification/regression head on top of the base BERT model allowing us to use it for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"neutral\", 1: \"positive\", 2: \"negative\"}\n",
    "model_name = 'bert-large-uncased'\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(model_name, id2label=id2label)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(model_name, config=bert_config)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "Data needs to be transformed to the format accepted by [HuggingFace Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) class with fields expected by our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.map(lambda e: bert_tokenizer(e['sentence'], truncation=True, padding='max_length', max_length=128), batched=True)\n",
    "dataset_val = dataset_val.map(lambda e: bert_tokenizer(e['sentence'], truncation=True, padding='max_length', max_length=128), batched=True)\n",
    "dataset_test = dataset_test.map(lambda e: bert_tokenizer(e['sentence'], truncation=True, padding='max_length' , max_length=128), batched=True)\n",
    "\n",
    "dataset_train = dataset_train.remove_columns(['sentence'])\n",
    "dataset_val = dataset_val.remove_columns(['sentence'])\n",
    "dataset_test = dataset_test.remove_columns(['sentence'])\n",
    "\n",
    "dataset_train.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataset_val.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataset_test.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Transformers library version from Habana's [Model Reference](https://github.com/HabanaAI/Model-References/tree/master/PyTorch/nlp/finetuning/huggingface/bert/#setup) repository is capable of training models on Gaudi cards with minimal code changes. There are a few new training arguments that are described [here](https://github.com/HabanaAI/Model-References/blob/master/PyTorch/nlp/finetuning/huggingface/bert/transformers/src/transformers/training_args.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy': accuracy_score(predictions, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "Enable use habana\n",
      "Single node run\n",
      "Enable habana lazy mode\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='temp/',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='no',\n",
    "    logging_strategy='epoch',\n",
    "    logging_dir='logs/',\n",
    "    report_to='tensorboard',\n",
    "\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model='accuracy',\n",
    "\n",
    "    use_habana=True,                   # use Habana device\n",
    "    use_lazy_mode=True,                # use Gaudi lazy mode\n",
    "    use_fused_adam=True,               # used optimised version of Adam for Gaudi\n",
    "    use_fused_clip_norm=True,          # use Habana's fused gradient norm clipping operator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to optimize the speed of training, you can use [Habana Mixed Precision](https://developer.habana.ai/tutorials/pytorch/pytorch-mixed-precision/). It requires you to specify which operations should run using bf16 vs. fp32 precision. Sample values can be taken from [Model Reference](https://github.com/HabanaAI/Model-References/tree/master/PyTorch/nlp/finetuning/huggingface/bert) or [Habana page on HuggingFace](https://huggingface.co/Habana)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "Enable use habana\n",
      "Single node run\n",
      "Enable habana lazy mode\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='temp/',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='no',\n",
    "    logging_strategy='epoch',\n",
    "    logging_dir='logs/',\n",
    "    report_to='tensorboard',\n",
    "\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model='accuracy',\n",
    "\n",
    "    use_habana=True,                   # use Habana device\n",
    "    use_lazy_mode=True,                # use Gaudi lazy mode\n",
    "    use_fused_adam=True,               # used optimised version of Adam for Gaudi\n",
    "    use_fused_clip_norm=True,          # use Habana's fused gradient norm clipping operator\n",
    "\n",
    "    # Optional model improvements\n",
    "    hmp=True,                          # use Habana mixed precision\n",
    "    hmp_bf16=\"hmp_bf16_ops\",           # file with list of operations performed in bf16\n",
    "    hmp_fp32=\"hmp_fp32_ops\",           # file with list of operations performed in fp32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabled lazy mode\n",
      "***** Running training *****\n",
      "  Num examples = 3924\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2455\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2455' max='2455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2455/2455 08:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210600</td>\n",
       "      <td>0.441805</td>\n",
       "      <td>0.803204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.142600</td>\n",
       "      <td>0.504630</td>\n",
       "      <td>0.846682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.580143</td>\n",
       "      <td>0.851259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.650701</td>\n",
       "      <td>0.848970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.670802</td>\n",
       "      <td>0.851259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 437\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 437\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 437\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 437\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 437\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2455, training_loss=0.09190003381484395, metrics={'train_runtime': 501.3593, 'train_samples_per_second': 39.134, 'train_steps_per_second': 4.897, 'total_flos': 4571138781987840.0, 'train_loss': 0.09190003381484395, 'epoch': 5.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert_model,                   # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=args,                          # training arguments, defined above\n",
    "    train_dataset=dataset_train,        # training dataset\n",
    "    eval_dataset=dataset_val,           # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--- ![finnet_full_diagram.png](finnet_full_diagram.svg) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Prediction results on train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 3924\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1103' max='981' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [981/981 01:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.01688225194811821,\n",
       " 'train_accuracy': 0.996177370030581,\n",
       " 'train_runtime': 67.0568,\n",
       " 'train_samples_per_second': 58.518,\n",
       " 'train_steps_per_second': 14.629}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(dataset_train, metric_key_prefix=\"train\").metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 485\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.6058623194694519,\n",
       " 'test_accuracy': 0.8515463917525773,\n",
       " 'test_runtime': 7.8169,\n",
       " 'test_samples_per_second': 62.045,\n",
       " 'test_steps_per_second': 15.607}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(dataset_test).metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Examples\n",
    "\n",
    "Here, we run our trained FinBERT model to recognize the sentiments of a few sample statements taken from the New York Times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'neutral', 'score': 0.9094224572181702}]\n",
      "[{'label': 'positive', 'score': 0.9752092957496643}]\n",
      "[{'label': 'negative', 'score': 0.8343048095703125}]\n",
      "[{'label': 'neutral', 'score': 0.5730178952217102}]\n",
      "[{'label': 'negative', 'score': 0.9138352274894714}]\n"
     ]
    }
   ],
   "source": [
    "pipe = TextClassificationPipeline(model=bert_model, tokenizer=bert_tokenizer)\n",
    "pipe.device=torch.device('hpu')\n",
    "\n",
    "print(pipe(\"Alabama Takes From the Poor and Gives to the Rich\"))\n",
    "print(pipe(\"Economists are predicting the highest rate of employment in 15 years\"))\n",
    "print(pipe(\"Itâ€™s Been a Poor Year So Far for Municipal Bonds\"))\n",
    "print(pipe(\"Lavish Money Laundering Schemes Exposed in Canada\"))\n",
    "print(pipe(\"Stocks edge lower as bank earnings add to concerns about the economy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning on Multiple Gaudis\n",
    "\n",
    "The simplest and efficient way of training transformers models on multiple Gaudi cards is by using `mpirun` command. All code cells from this notebook were copied to [finbert.py](finbert.py) python file. In order to run training on multiple cards, please use commands:\n",
    "```\n",
    "export MASTER_ADDR=\"localhost\"\n",
    "export MASTER_PORT=\"12345\"\n",
    "mpirun -n 8 --bind-to core --map-by socket:PE=7 --rank-by core --report-bindings --allow-run-as-root python3 finbert.py\n",
    "\n",
    "```\n",
    "Additional examples on distributed training of transformer models can be found in [Model Reference](https://github.com/HabanaAI/Model-References/tree/master/PyTorch/nlp/finetuning/huggingface/bert#multicard-training). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
